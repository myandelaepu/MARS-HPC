# -*- coding: utf-8 -*-
"""MARS-HPC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CQOQsCC6UPSzh4Sddg4LIt9-OSGqbcmv
"""

!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
!pip install numpy pandas matplotlib seaborn scikit-learn scipy
!pip install optuna plotly

import os
import gzip
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from scipy import stats
import pickle
import random
from collections import deque, namedtuple
import warnings
warnings.filterwarnings('ignore')

def set_seed(seed=42):
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

class HPC_DataProcessor:

    def __init__(self):
        self.scalers = {}
        self.energy_scaler = StandardScaler()
        self.perf_scaler = StandardScaler()
        self.resilience_scaler = StandardScaler()

    def load_hpc_data(self, filename):
        try:
            if filename.endswith('.gz'):
                with gzip.open(filename, 'rt') as f:
                    df = pd.read_csv(f)
            else:
                df = pd.read_csv(filename)
            print(f"Loaded {filename}: {df.shape}")
            return df
        except Exception as e:
            print(f"Error loading {filename}: {e}")
            return None

    def extract_features(self, df):
        features = {}

        required_cols = {
            'USED_CORE_HOURS': 0,
            'RUNTIME_SECONDS': 3600,
            'NODES_USED': 1,
            'CORES_USED': 1,
            'WALLTIME_SECONDS': 3600,
            'QUEUED_WAIT_SECONDS': 0,
            'ELIGIBLE_WAIT_SECONDS': 0,
            'NODES_REQUESTED': 1,
            'CORES_REQUESTED': 1,
            'EXIT_STATUS': 0,
            'EXIT_CODE': 0,
            'OVERBURN_CORE_HOURS': 0,
            'IS_OVERBURN': 0,
            'QUEUED_DATE_ID': 20240101,
            'START_DATE_ID': 20240101,
            'END_DATE_ID': 20240101
        }

        for col, default_val in required_cols.items():
            if col not in df.columns:
                df[col] = default_val
            else:
                df[col] = df[col].fillna(default_val)

        energy_cols = ['USED_CORE_HOURS', 'RUNTIME_SECONDS', 'NODES_USED', 'CORES_USED']
        features['energy'] = df[energy_cols]

        perf_cols = ['WALLTIME_SECONDS', 'RUNTIME_SECONDS', 'QUEUED_WAIT_SECONDS',
                    'ELIGIBLE_WAIT_SECONDS', 'NODES_REQUESTED', 'CORES_REQUESTED']
        features['performance'] = df[perf_cols]

        resilience_cols = ['EXIT_STATUS', 'EXIT_CODE', 'OVERBURN_CORE_HOURS', 'IS_OVERBURN']
        features['resilience'] = df[resilience_cols]

        features['context'] = pd.DataFrame({
            'queue_time': (df['QUEUED_WAIT_SECONDS'] / 3600.0).fillna(0),  # Hours
            'runtime_ratio': (df['RUNTIME_SECONDS'] / df['WALLTIME_SECONDS'].replace(0, 1)).fillna(1.0),
            'resource_ratio': (df['CORES_USED'] / df['CORES_REQUESTED'].replace(0, 1)).fillna(1.0)
        })

        return features

    def compute_rewards(self, df):
        rewards = {}

        energy_consumption = df['USED_CORE_HOURS'].fillna(0)
        rewards['energy'] = -np.log1p(energy_consumption)

        runtime = df['RUNTIME_SECONDS'].fillna(df['RUNTIME_SECONDS'].median())
        wait_time = df['QUEUED_WAIT_SECONDS'].fillna(0)
        turnaround_time = runtime + wait_time
        rewards['performance'] = -np.log1p(turnaround_time)

        exit_success = (df['EXIT_STATUS'] == 0).astype(float)
        overburn_penalty = df['IS_OVERBURN'].fillna(0) * 0.1
        rewards['resilience'] = exit_success - overburn_penalty

        return rewards

    def create_state_representation(self, features):
        state_components = []
        total_features = 0

        for key, feature_df in features.items():
            if key not in self.scalers:
                self.scalers[key] = StandardScaler()

            feature_array = np.array(feature_df)
            if len(feature_array.shape) == 1:
                feature_array = feature_array.reshape(-1, 1)

            scaled_features = self.scalers[key].fit_transform(feature_array)
            state_components.append(scaled_features)
            total_features += scaled_features.shape[1]

        state = np.concatenate(state_components, axis=1)

        print(f"State representation created: {state.shape}, Total features: {total_features}")
        return state, total_features

class MultiObjectiveExperienceReplay:

    def __init__(self, capacity=100000):
        self.capacity = capacity
        self.energy_buffer = deque(maxlen=capacity)
        self.performance_buffer = deque(maxlen=capacity)
        self.resilience_buffer = deque(maxlen=capacity)
        self.pareto_archive = []
        self.experience = namedtuple('Experience',
                                   ['state', 'action', 'reward_vector', 'next_state', 'done'])

    def add(self, state, action, reward_vector, next_state, done):
        exp = self.experience(state, action, reward_vector, next_state, done)

        energy_priority = abs(reward_vector[0]) + np.random.normal(0, 0.01)
        perf_priority = abs(reward_vector[1]) + np.random.normal(0, 0.01)
        resilience_priority = abs(reward_vector[2]) + np.random.normal(0, 0.01)

        self.energy_buffer.append((exp, energy_priority))
        self.performance_buffer.append((exp, perf_priority))
        self.resilience_buffer.append((exp, resilience_priority))

        self.update_pareto_archive(reward_vector)

    def update_pareto_archive(self, new_reward):
        dominated = []
        for i, archived_reward in enumerate(self.pareto_archive):
            if self.dominates(new_reward, archived_reward):
                dominated.append(i)
            elif self.dominates(archived_reward, new_reward):
                return

        for i in reversed(dominated):
            del self.pareto_archive[i]

        self.pareto_archive.append(new_reward)

    def dominates(self, a, b):
        return all(a[i] >= b[i] for i in range(len(a))) and any(a[i] > b[i] for i in range(len(a)))

    def sample(self, batch_size, objective_weights):
        samples = []

        energy_samples = int(batch_size * objective_weights[0])
        perf_samples = int(batch_size * objective_weights[1])
        resilience_samples = batch_size - energy_samples - perf_samples

        if len(self.energy_buffer) > 0:
            energy_batch = random.sample(self.energy_buffer,
                                       min(energy_samples, len(self.energy_buffer)))
            samples.extend([exp for exp, _ in energy_batch])

        if len(self.performance_buffer) > 0:
            perf_batch = random.sample(self.performance_buffer,
                                     min(perf_samples, len(self.performance_buffer)))
            samples.extend([exp for exp, _ in perf_batch])

        if len(self.resilience_buffer) > 0:
            resilience_batch = random.sample(self.resilience_buffer,
                                           min(resilience_samples, len(self.resilience_buffer)))
            samples.extend([exp for exp, _ in resilience_batch])

        return samples

    def __len__(self):
        return min(len(self.energy_buffer), len(self.performance_buffer), len(self.resilience_buffer))

class ResilienceAwareDQN(nn.Module):

    def __init__(self, state_dim, action_dim, hidden_dims=[1024, 512, 256, 128]):
        super(ResilienceAwareDQN, self).__init__()

        self.state_dim = state_dim
        self.action_dim = action_dim

        layers = []
        prev_dim = state_dim
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.1)
            ])
            prev_dim = hidden_dim

        self.shared_layers = nn.Sequential(*layers)

        final_dim = hidden_dims[-1]

        self.energy_head = nn.Sequential(
            nn.Linear(final_dim, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )

        self.performance_head = nn.Sequential(
            nn.Linear(final_dim, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )

        self.resilience_head = nn.Sequential(
            nn.Linear(final_dim, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )

        self.attention = nn.MultiheadAttention(final_dim, num_heads=8, batch_first=True)

    def forward(self, state, objective_weights=None):
        if objective_weights is None:
            objective_weights = torch.tensor([1/3, 1/3, 1/3], device=state.device)

        shared_features = self.shared_layers(state)

        if len(shared_features.shape) == 2:
            shared_features_att = shared_features.unsqueeze(1)
            attended_features, _ = self.attention(shared_features_att, shared_features_att, shared_features_att)
            attended_features = attended_features.squeeze(1)
        else:
            attended_features, _ = self.attention(shared_features, shared_features, shared_features)

        q_energy = self.energy_head(attended_features)
        q_performance = self.performance_head(attended_features)
        q_resilience = self.resilience_head(attended_features)

        q_combined = (objective_weights[0] * q_energy +
                     objective_weights[1] * q_performance +
                     objective_weights[2] * q_resilience)

        return q_combined, (q_energy, q_performance, q_resilience)

class MARSHPCAgent:

    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, epsilon=1.0, epsilon_decay=0.995):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = 0.01

        self.q_network = ResilienceAwareDQN(state_dim, action_dim).to(device)
        self.target_network = ResilienceAwareDQN(state_dim, action_dim).to(device)
        self.optimizer = optim.AdamW(self.q_network.parameters(), lr=lr, weight_decay=1e-4)

        self.update_target_network()

        self.memory = MultiObjectiveExperienceReplay(capacity=100000)

        self.objective_weights = torch.tensor([1/3, 1/3, 1/3], device=device)

        self.losses = []
        self.rewards_history = {'energy': [], 'performance': [], 'resilience': []}

    def update_target_network(self):
        self.target_network.load_state_dict(self.q_network.state_dict())

    def select_action(self, state, training=True):
        if training and np.random.random() < self.epsilon:
            return np.random.randint(self.action_dim)

        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
            q_values, _ = self.q_network(state_tensor, self.objective_weights)
            action = q_values.argmax().item()

        return action

    def store_transition(self, state, action, reward_vector, next_state, done):
        self.memory.add(state, action, reward_vector, next_state, done)

    def compute_multi_objective_loss(self, q_values, target_values, objective_weights):
        q_energy, q_performance, q_resilience = q_values
        target_energy, target_performance, target_resilience = target_values

        loss_energy = F.mse_loss(q_energy, target_energy)
        loss_performance = F.mse_loss(q_performance, target_performance)
        loss_resilience = F.mse_loss(q_resilience, target_resilience)

        total_loss = (objective_weights[0] * loss_energy +
                     objective_weights[1] * loss_performance +
                     objective_weights[2] * loss_resilience)

        return total_loss, (loss_energy, loss_performance, loss_resilience)

    def train(self, batch_size=128):
        if len(self.memory) < batch_size:
            return

        experiences = self.memory.sample(batch_size, self.objective_weights.cpu().numpy())

        if not experiences:
            return

        states = torch.FloatTensor([e.state for e in experiences]).to(device)
        actions = torch.LongTensor([e.action for e in experiences]).to(device)
        reward_vectors = torch.FloatTensor([e.reward_vector for e in experiences]).to(device)
        next_states = torch.FloatTensor([e.next_state for e in experiences]).to(device)
        dones = torch.BoolTensor([e.done for e in experiences]).to(device)

        current_q, current_q_components = self.q_network(states, self.objective_weights)
        current_q_values = current_q.gather(1, actions.unsqueeze(1))

        with torch.no_grad():
            next_q, next_q_components = self.target_network(next_states, self.objective_weights)
            next_q_values = next_q.max(1)[0].detach()

            targets = []
            for i in range(3):
                target_i = reward_vectors[:, i] + (self.gamma * next_q_values * ~dones)
                targets.append(target_i.unsqueeze(1))

            target_values = tuple(targets)

        current_q_individual = []
        for i, q_component in enumerate(current_q_components):
            current_q_individual.append(q_component.gather(1, actions.unsqueeze(1)))

        loss, individual_losses = self.compute_multi_objective_loss(
            tuple(current_q_individual), target_values, self.objective_weights
        )

        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=1.0)
        self.optimizer.step()

        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)

        self.losses.append(loss.item())

        return loss.item()

    def update_objective_weights(self, performance_metrics):
        energy_weight = 0.4 if performance_metrics.get('energy_efficiency', 0.5) < 0.5 else 0.3
        perf_weight = 0.4 if performance_metrics.get('avg_turnaround', 1.0) > 0.8 else 0.3
        resilience_weight = 1.0 - energy_weight - perf_weight

        self.objective_weights = torch.tensor([energy_weight, perf_weight, resilience_weight], device=device)

class HPC_Simulator:

    def __init__(self, data_processor, num_nodes=1000, state_dim=17):
        self.data_processor = data_processor
        self.num_nodes = num_nodes
        self.state_dim = state_dim
        self.current_jobs = []
        self.completed_jobs = []
        self.system_metrics = {
            'energy_consumption': 0.0,
            'total_runtime': 0.0,
            'failed_jobs': 0,
            'successful_jobs': 0,
            'queue_length': 0,
            'avg_wait_time': 0,
            'system_utilization': 0,
            'avg_core_hours': 0,
            'avg_runtime': 0,
            'failure_rate': 0,
            'throughput': 0,
            'load_balance': 0.5,
            'power_efficiency': 0.5,
            'resource_fragmentation': 0.5,
            'network_utilization': 0.5,
            'memory_utilization': 0.5
        }

    def reset(self):
        self.current_jobs = []
        self.completed_jobs = []
        self.system_metrics = {
            'energy_consumption': 0.0,
            'total_runtime': 0.0,
            'failed_jobs': 0,
            'successful_jobs': 0,
            'queue_length': 0,
            'avg_wait_time': 0,
            'system_utilization': 0,
            'avg_core_hours': 0,
            'avg_runtime': 0,
            'failure_rate': 0,
            'throughput': 0,
            'load_balance': 0.5,
            'power_efficiency': 0.5,
            'resource_fragmentation': 0.5,
            'network_utilization': 0.5,
            'memory_utilization': 0.5
        }
        return self._get_state()

    def _get_state(self):
        queue_length = len(self.current_jobs) / 100.0
        avg_wait_time = np.mean([job.get('wait_time', 0) for job in self.current_jobs]) / 3600.0 if self.current_jobs else 0
        system_utilization = min(len(self.current_jobs) / self.num_nodes, 1.0)
        energy_consumption = self.system_metrics['energy_consumption'] / 10000.0
        completed_jobs = len(self.completed_jobs) / 1000.0

        avg_core_hours = np.mean([job.get('core_hours', 100) for job in self.current_jobs]) / 1000.0 if self.current_jobs else 0
        avg_runtime = np.mean([job.get('runtime', 3600) for job in self.current_jobs]) / 10000.0 if self.current_jobs else 0

        failure_rate = self.system_metrics['failed_jobs'] / max(1, self.system_metrics['failed_jobs'] + self.system_metrics['successful_jobs'])
        throughput = self.system_metrics['successful_jobs'] / 100.0

        load_balance = self.system_metrics.get('load_balance', 0.5)
        power_efficiency = self.system_metrics.get('power_efficiency', 0.5)
        resource_fragmentation = self.system_metrics.get('resource_fragmentation', 0.5)
        network_utilization = self.system_metrics.get('network_utilization', 0.5)
        memory_utilization = self.system_metrics.get('memory_utilization', 0.5)

        queue_variance = np.var([job.get('core_hours', 100) for job in self.current_jobs]) / 10000.0 if len(self.current_jobs) > 1 else 0
        avg_nodes_requested = np.mean([job.get('nodes', 1) for job in self.current_jobs]) / 10.0 if self.current_jobs else 0

        current_time_normalized = (len(self.completed_jobs) % 24) / 24.0
        workload_intensity = len(self.current_jobs) / 50.0

        state = np.array([
            queue_length,
            avg_wait_time,
            system_utilization,
            energy_consumption,
            completed_jobs,
            avg_core_hours,
            avg_runtime,
            failure_rate,
            throughput,
            load_balance,
            power_efficiency,
            resource_fragmentation,
            network_utilization,
            memory_utilization,
            queue_variance,
            avg_nodes_requested,
            current_time_normalized
        ])

        if len(state) != self.state_dim:
            if len(state) < self.state_dim:
                state = np.pad(state, (0, self.state_dim - len(state)), 'constant', constant_values=0)
            else:
                state = state[:self.state_dim]

        return state

    def step(self, action):

        if self.current_jobs:
            job = self.current_jobs.pop(0)

            if action == 0:
                energy_mult = 0.8
                performance_mult = 1.2
                failure_prob = 0.05
            else:
                energy_mult = 1.3
                performance_mult = 0.8
                failure_prob = 0.15

            energy_reward = -job.get('core_hours', 100) * energy_mult / 1000.0
            performance_reward = -job.get('runtime', 3600) * performance_mult / 3600.0

            success = np.random.random() > failure_prob
            resilience_reward = 1.0 if success else -1.0

            self.system_metrics['energy_consumption'] += abs(energy_reward * 1000)
            self.system_metrics['total_runtime'] += abs(performance_reward * 3600)

            if success:
                self.system_metrics['successful_jobs'] += 1
                self.completed_jobs.append(job)
            else:
                self.system_metrics['failed_jobs'] += 1

            self.system_metrics['load_balance'] = np.random.uniform(0.3, 0.9)
            self.system_metrics['power_efficiency'] = np.random.uniform(0.4, 0.8)
            self.system_metrics['resource_fragmentation'] = np.random.uniform(0.2, 0.7)
            self.system_metrics['network_utilization'] = np.random.uniform(0.1, 0.9)
            self.system_metrics['memory_utilization'] = np.random.uniform(0.3, 0.8)

            reward_vector = [energy_reward, performance_reward, resilience_reward]
            done = len(self.current_jobs) == 0

            return self._get_state(), reward_vector, done

        else:
            return self._get_state(), [0, 0, 0], True

    def add_jobs(self, job_data):
        for _, job in job_data.iterrows():
            self.current_jobs.append({
                'core_hours': job.get('USED_CORE_HOURS', 100),
                'runtime': job.get('RUNTIME_SECONDS', 3600),
                'wait_time': job.get('QUEUED_WAIT_SECONDS', 0),
                'nodes': job.get('NODES_USED', 1)
            })

def train_mars_hpc(datasets, num_episodes=1000):
    data_processor = HPC_DataProcessor()

    all_data = []
    for dataset_file in datasets:
        df = data_processor.load_hpc_data(dataset_file)
        if df is not None:
            sample_size = min(10000, len(df))
            df_sample = df.sample(n=sample_size, random_state=42)
            all_data.append(df_sample)

    combined_df = pd.concat(all_data, ignore_index=True)
    print(f"Combined dataset shape: {combined_df.shape}")

    features = data_processor.extract_features(combined_df)
    rewards = data_processor.compute_rewards(combined_df)
    state_representation, total_features = data_processor.create_state_representation(features)

    action_dim = 2

    print(f"Initializing agent with state_dim={total_features}, action_dim={action_dim}")
    agent = MARSHPCAgent(state_dim=total_features, action_dim=action_dim)
    simulator = HPC_Simulator(data_processor)

    episode_rewards = {'energy': [], 'performance': [], 'resilience': [], 'total': []}
    pareto_solutions = []

    print("Starting MARS-HPC training...")

    for episode in range(num_episodes):
        episode_jobs = combined_df.sample(n=min(50, len(combined_df)), replace=True)
        simulator.add_jobs(episode_jobs)

        state = simulator.reset()
        total_reward = [0, 0, 0]
        step_count = 0

        while step_count < 50:
            action = agent.select_action(state)
            next_state, reward_vector, done = simulator.step(action)

            agent.store_transition(state, action, reward_vector, next_state, done)

            if len(agent.memory) > 128:
                loss = agent.train()

            total_reward = [total_reward[i] + reward_vector[i] for i in range(3)]
            state = next_state
            step_count += 1

            if done:
                break

        episode_rewards['energy'].append(total_reward[0])
        episode_rewards['performance'].append(total_reward[1])
        episode_rewards['resilience'].append(total_reward[2])
        episode_rewards['total'].append(sum(total_reward))

        pareto_solutions.append(total_reward.copy())

        if episode % 100 == 0:
            agent.update_target_network()

            avg_reward = np.mean(episode_rewards['total'][-100:])
            avg_energy = np.mean(episode_rewards['energy'][-100:])
            avg_performance = np.mean(episode_rewards['performance'][-100:])
            avg_resilience = np.mean(episode_rewards['resilience'][-100:])

            print(f"Episode {episode}: Avg Reward: {avg_reward:.3f}, "
                  f"Energy: {avg_energy:.3f}, Performance: {avg_performance:.3f}, "
                  f"Resilience: {avg_resilience:.3f}, Epsilon: {agent.epsilon:.3f}")

    return agent, episode_rewards, pareto_solutions, data_processor

def evaluate_mars_hpc(agent, test_data, data_processor, num_episodes=100):

    simulator = HPC_Simulator(data_processor)
    results = {
        'energy_consumption': [],
        'job_completion_time': [],
        'system_availability': [],
        'throughput': []
    }

    for episode in range(num_episodes):
        episode_jobs = test_data.sample(n=min(50, len(test_data)), replace=True)
        simulator.add_jobs(episode_jobs)

        state = simulator.reset()
        episode_metrics = {
            'energy': 0,
            'completion_time': 0,
            'failures': 0,
            'successes': 0
        }

        step_count = 0
        while step_count < 25:
            action = agent.select_action(state, training=False)
            next_state, reward_vector, done = simulator.step(action)

            episode_metrics['energy'] += abs(reward_vector[0])
            episode_metrics['completion_time'] += abs(reward_vector[1])

            if reward_vector[2] > 0:
                episode_metrics['successes'] += 1
            else:
                episode_metrics['failures'] += 1

            state = next_state
            step_count += 1

            if done:
                break

        results['energy_consumption'].append(episode_metrics['energy'])
        results['job_completion_time'].append(episode_metrics['completion_time'])

        total_jobs = episode_metrics['successes'] + episode_metrics['failures']
        availability = episode_metrics['successes'] / total_jobs if total_jobs > 0 else 1.0
        results['system_availability'].append(availability)
        results['throughput'].append(episode_metrics['successes'])

    return results

def compute_statistics_and_significance(results, baseline_results=None):
    stats_results = {}

    for metric, values in results.items():
        values = np.array(values)
        stats_results[metric] = {
            'mean': np.mean(values),
            'std': np.std(values),
            'median': np.median(values),
            'ci_lower': np.percentile(values, 2.5),
            'ci_upper': np.percentile(values, 97.5)
        }

        if baseline_results and metric in baseline_results:
            baseline_values = np.array(baseline_results[metric])

            t_stat, p_value = stats.ttest_ind(values, baseline_values, alternative='two-sided')

            pooled_std = np.sqrt((np.var(values) + np.var(baseline_values)) / 2)
            cohens_d = (np.mean(values) - np.mean(baseline_values)) / pooled_std

            stats_results[metric]['t_statistic'] = t_stat
            stats_results[metric]['p_value'] = p_value
            stats_results[metric]['cohens_d'] = cohens_d
            stats_results[metric]['significant'] = p_value < 0.05

    return stats_results

def visualize_results(episode_rewards, pareto_solutions, results=None):
    """Visualize training and evaluation results"""

    plt.style.use('seaborn-v0_8')
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))

    axes[0, 0].plot(episode_rewards['energy'], label='Energy', alpha=0.7)
    axes[0, 0].plot(episode_rewards['performance'], label='Performance', alpha=0.7)
    axes[0, 0].plot(episode_rewards['resilience'], label='Resilience', alpha=0.7)
    axes[0, 0].set_title('Training Rewards Over Episodes')
    axes[0, 0].set_xlabel('Episode')
    axes[0, 0].set_ylabel('Reward')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)

    pareto_array = np.array(pareto_solutions)
    if pareto_array.shape[1] >= 3:
        ax = fig.add_subplot(2, 2, 2, projection='3d')
        ax.scatter(pareto_array[:, 0], pareto_array[:, 1], pareto_array[:, 2],
                  c=range(len(pareto_array)), cmap='viridis', alpha=0.6)
        ax.set_xlabel('Energy')
        ax.set_ylabel('Performance')
        ax.set_zlabel('Resilience')
        ax.set_title('Pareto Frontier Evolution')
    else:
        axes[0, 1].scatter(pareto_array[:, 0], pareto_array[:, 1], alpha=0.6)
        axes[0, 1].set_xlabel('Energy')
        axes[0, 1].set_ylabel('Performance')
        axes[0, 1].set_title('Pareto Frontier (2D)')
        axes[0, 1].grid(True, alpha=0.3)

    if results:
        metrics = list(results.keys())
        values = [np.mean(results[metric]) for metric in metrics]
        errors = [np.std(results[metric]) for metric in metrics]

        axes[1, 0].bar(metrics, values, yerr=errors, capsize=5, alpha=0.7)
        axes[1, 0].set_title('Evaluation Metrics')
        axes[1, 0].set_ylabel('Performance')
        axes[1, 0].tick_params(axis='x', rotation=45)
        axes[1, 0].grid(True, alpha=0.3)

    if hasattr(episode_rewards, 'losses'):
        axes[1, 1].plot(episode_rewards['losses'], alpha=0.7)
        axes[1, 1].set_title('Training Loss Convergence')
        axes[1, 1].set_xlabel('Training Step')
        axes[1, 1].set_ylabel('Loss')
        axes[1, 1].grid(True, alpha=0.3)
    else:
        axes[1, 1].hist(episode_rewards['total'], bins=50, alpha=0.7)
        axes[1, 1].set_title('Total Reward Distribution')
        axes[1, 1].set_xlabel('Total Reward')
        axes[1, 1].set_ylabel('Frequency')
        axes[1, 1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

def create_performance_comparison_table(mars_results, baseline_results=None):
    mars_metrics = {
        'Energy (kWh)': np.mean(mars_results['energy_consumption']) * 1000,  # Convert to kWh
        'Performance (h)': np.mean(mars_results['job_completion_time']) * 24,  # Convert to hours
        'Resilience (%)': np.mean(mars_results['system_availability']) * 100,
        'Throughput': np.mean(mars_results['throughput'])
    }

    comparison_data = {
        'Algorithm': ['MARS-HPC'],
        'Energy (kWh)': [f"{mars_metrics['Energy (kWh)']:.1f}±{np.std(mars_results['energy_consumption']) * 1000:.1f}"],
        'Performance (h)': [f"{mars_metrics['Performance (h)']:.1f}±{np.std(mars_results['job_completion_time']) * 24:.1f}"],
        'Resilience (%)': [f"{mars_metrics['Resilience (%)']:.1f}±{np.std(mars_results['system_availability']) * 100:.1f}"],
        'Throughput': [f"{mars_metrics['Throughput']:.1f}±{np.std(mars_results['throughput']):.1f}"]
    }

    if baseline_results:
        for baseline_name, baseline_data in baseline_results.items():
            comparison_data['Algorithm'].append(baseline_name)
            comparison_data['Energy (kWh)'].append(
                f"{np.mean(baseline_data['energy_consumption']) * 1000:.1f}±{np.std(baseline_data['energy_consumption']) * 1000:.1f}"
            )
            comparison_data['Performance (h)'].append(
                f"{np.mean(baseline_data['job_completion_time']) * 24:.1f}±{np.std(baseline_data['job_completion_time']) * 24:.1f}"
            )
            comparison_data['Resilience (%)'].append(
                f"{np.mean(baseline_data['system_availability']) * 100:.1f}±{np.std(baseline_data['system_availability']) * 100:.1f}"
            )
            comparison_data['Throughput'].append(
                f"{np.mean(baseline_data['throughput']):.1f}±{np.std(baseline_data['throughput']):.1f}"
            )

    df_comparison = pd.DataFrame(comparison_data)
    return df_comparison

def run_baseline_comparison(test_data, data_processor):

    baselines = {}
    print("Running SLURM baseline...")
    slurm_results = simulate_baseline_scheduler(test_data, strategy='fifo')
    baselines['SLURM'] = slurm_results

    print("Running Aggressive baseline...")
    aggressive_results = simulate_baseline_scheduler(test_data, strategy='aggressive')
    baselines['Aggressive'] = aggressive_results

    print("Running Energy-Aware baseline...")
    energy_results = simulate_baseline_scheduler(test_data, strategy='energy_aware')
    baselines['Energy-Aware'] = energy_results

    return baselines

def simulate_baseline_scheduler(test_data, strategy='fifo', num_episodes=100):

    results = {
        'energy_consumption': [],
        'job_completion_time': [],
        'system_availability': [],
        'throughput': []
    }

    for episode in range(num_episodes):
        episode_jobs = test_data.sample(n=min(50, len(test_data)), replace=True)

        episode_metrics = {
            'energy': 0,
            'completion_time': 0,
            'failures': 0,
            'successes': 0
        }

        for _, job in episode_jobs.iterrows():
            core_hours = job.get('USED_CORE_HOURS', 100)
            runtime = job.get('RUNTIME_SECONDS', 3600)

            if strategy == 'fifo':
                energy_mult = 1.0
                performance_mult = 1.0
                failure_rate = 0.1
            elif strategy == 'aggressive':
                energy_mult = 1.5
                performance_mult = 0.7
                failure_rate = 0.2
            elif strategy == 'energy_aware':
                energy_mult = 0.8
                performance_mult = 1.3
                failure_rate = 0.08
            else:
                energy_mult = 1.0
                performance_mult = 1.0
                failure_rate = 0.1

            energy_consumed = core_hours * energy_mult / 1000.0
            completion_time = runtime * performance_mult / 3600.0

            episode_metrics['energy'] += energy_consumed
            episode_metrics['completion_time'] += completion_time

            if np.random.random() > failure_rate:
                episode_metrics['successes'] += 1
            else:
                episode_metrics['failures'] += 1

        results['energy_consumption'].append(episode_metrics['energy'])
        results['job_completion_time'].append(episode_metrics['completion_time'])

        total_jobs = episode_metrics['successes'] + episode_metrics['failures']
        availability = episode_metrics['successes'] / total_jobs if total_jobs > 0 else 1.0
        results['system_availability'].append(availability)
        results['throughput'].append(episode_metrics['successes'])

    return results

def main():

    print("="*80)
    print("MARS-HPC: Multi-Objective Adaptive RL-Scheduler")
    print("Implementation for Google Colab with ALCF Production Datasets")
    print("="*80)

    datasets = [
        'ANL-ALCF-DJC-POLARIS_20240101_20241031.csv.gz',
        'ANL-ALCF-DJC-MIRA_20190101_20191231.csv.gz',
        'ANL-ALCF-DJC-COOLEY_20190101_20191231.csv.gz'
    ]

    existing_datasets = []
    for dataset in datasets:
        if os.path.exists(dataset):
            existing_datasets.append(dataset)
            print(f"Found dataset: {dataset}")
        else:
            print(f"Dataset not found: {dataset}")

    if not existing_datasets:
        print("Error: No datasets found. Please upload the ALCF datasets to the current directory.")
        return

    print(f"\nUsing {len(existing_datasets)} datasets for training and evaluation.")

    print("\n" + "="*60)
    print("Phase 1: Training MARS-HPC Agent")
    print("="*60)

    agent, episode_rewards, pareto_solutions, data_processor = train_mars_hpc(
        existing_datasets,
        num_episodes=1000
    )

    print("Training completed successfully!")

    print("\n" + "="*60)
    print("Phase 2: Evaluation and Comparison")
    print("="*60)

    test_data_list = []
    for dataset_file in existing_datasets:
        df = data_processor.load_hpc_data(dataset_file)
        if df is not None:
            test_data_list.append(df.sample(n=min(1000, len(df))))

    test_data = pd.concat(test_data_list, ignore_index=True)
    print(f"Test data prepared: {test_data.shape[0]} samples")

    mars_results = evaluate_mars_hpc(agent, test_data, data_processor, num_episodes=100)
    print("MARS-HPC evaluation completed!")

    baseline_results = run_baseline_comparison(test_data, data_processor)
    print("Baseline evaluations completed!")

    print("\n" + "="*60)
    print("Phase 3: Statistical Analysis")
    print("="*60)

    stats_results = {}
    for baseline_name, baseline_data in baseline_results.items():
        stats_results[baseline_name] = compute_statistics_and_significance(mars_results, baseline_data)

    # Display results
    print("\n" + "="*60)
    print("MARS-HPC Performance Results")
    print("="*60)

    print("\nMARS-HPC Metrics:")
    for metric, values in mars_results.items():
        mean_val = np.mean(values)
        std_val = np.std(values)
        print(f"{metric:25}: {mean_val:.4f} ± {std_val:.4f}")

    comparison_table = create_performance_comparison_table(mars_results, baseline_results)
    print("\nPerformance Comparison Table:")
    print(comparison_table.to_string(index=False))

    print("\nStatistical Significance Analysis:")
    for baseline_name, stats in stats_results.items():
        print(f"\nMARS-HPC vs {baseline_name}:")
        for metric, metric_stats in stats.items():
            if 'p_value' in metric_stats:
                significance = "***" if metric_stats['p_value'] < 0.001 else "**" if metric_stats['p_value'] < 0.01 else "*" if metric_stats['p_value'] < 0.05 else "ns"
                print(f"  {metric:20}: p={metric_stats['p_value']:.6f} {significance}, Cohen's d={metric_stats['cohens_d']:.3f}")

    print("\n" + "="*60)
    print("Improvement Analysis")
    print("="*60)

    for baseline_name, baseline_data in baseline_results.items():
        print(f"\nMARS-HPC vs {baseline_name}:")

        mars_energy = np.mean(mars_results['energy_consumption'])
        baseline_energy = np.mean(baseline_data['energy_consumption'])
        energy_improvement = ((baseline_energy - mars_energy) / baseline_energy) * 100
        print(f"  Energy Efficiency    : {energy_improvement:+.1f}%")

        mars_perf = np.mean(mars_results['job_completion_time'])
        baseline_perf = np.mean(baseline_data['job_completion_time'])
        perf_improvement = ((baseline_perf - mars_perf) / baseline_perf) * 100
        print(f"  Performance          : {perf_improvement:+.1f}%")

        mars_resilience = np.mean(mars_results['system_availability'])
        baseline_resilience = np.mean(baseline_data['system_availability'])
        resilience_improvement = ((mars_resilience - baseline_resilience) / baseline_resilience) * 100
        print(f"  System Resilience    : {resilience_improvement:+.1f}%")

        mars_throughput = np.mean(mars_results['throughput'])
        baseline_throughput = np.mean(baseline_data['throughput'])
        throughput_improvement = ((mars_throughput - baseline_throughput) / baseline_throughput) * 100
        print(f"  Throughput           : {throughput_improvement:+.1f}%")

    print("\n" + "="*60)
    print("Phase 4: Visualization")
    print("="*60)

    visualize_results(episode_rewards, pareto_solutions, mars_results)

    plt.figure(figsize=(15, 10))

    plt.subplot(2, 3, 1)
    algorithms = ['MARS-HPC'] + list(baseline_results.keys())
    energy_means = [np.mean(mars_results['energy_consumption'])] + [np.mean(baseline_results[name]['energy_consumption']) for name in baseline_results.keys()]
    plt.bar(algorithms, energy_means, alpha=0.7)
    plt.title('Energy Consumption Comparison')
    plt.ylabel('Energy (normalized)')
    plt.xticks(rotation=45)

    plt.subplot(2, 3, 2)
    perf_means = [np.mean(mars_results['job_completion_time'])] + [np.mean(baseline_results[name]['job_completion_time']) for name in baseline_results.keys()]
    plt.bar(algorithms, perf_means, alpha=0.7)
    plt.title('Job Completion Time Comparison')
    plt.ylabel('Time (normalized)')
    plt.xticks(rotation=45)

    plt.subplot(2, 3, 3)
    availability_means = [np.mean(mars_results['system_availability'])] + [np.mean(baseline_results[name]['system_availability']) for name in baseline_results.keys()]
    plt.bar(algorithms, availability_means, alpha=0.7)
    plt.title('System Availability Comparison')
    plt.ylabel('Availability (%)')
    plt.xticks(rotation=45)

    plt.subplot(2, 3, 4)
    all_energy_data = [mars_results['energy_consumption']] + [baseline_results[name]['energy_consumption'] for name in baseline_results.keys()]
    plt.boxplot(all_energy_data, labels=algorithms)
    plt.title('Energy Distribution')
    plt.xticks(rotation=45)

    plt.subplot(2, 3, 5)
    all_perf_data = [mars_results['job_completion_time']] + [baseline_results[name]['job_completion_time'] for name in baseline_results.keys()]
    plt.boxplot(all_perf_data, labels=algorithms)
    plt.title('Performance Distribution')
    plt.xticks(rotation=45)

    plt.subplot(2, 3, 6)
    all_availability_data = [mars_results['system_availability']] + [baseline_results[name]['system_availability'] for name in baseline_results.keys()]
    plt.boxplot(all_availability_data, labels=algorithms)
    plt.title('Availability Distribution')
    plt.xticks(rotation=45)

    plt.tight_layout()
    plt.show()

    results_dict = {
        'mars_results': mars_results,
        'baseline_results': baseline_results,
        'episode_rewards': episode_rewards,
        'pareto_solutions': pareto_solutions,
        'statistical_analysis': stats_results
    }

    with open('mars_hpc_results.pkl', 'wb') as f:
        pickle.dump(results_dict, f)

    torch.save(agent.q_network.state_dict(), 'mars_hpc_model.pth')

    print("\nResults saved to:")
    print("- mars_hpc_results.pkl (comprehensive results)")
    print("- mars_hpc_model.pth (trained model)")

    print("\n" + "="*60)
    print("MARS-HPC Implementation Completed Successfully!")
    print("="*60)

    return agent, mars_results, baseline_results, stats_results

if __name__ == "__main__":
    main()